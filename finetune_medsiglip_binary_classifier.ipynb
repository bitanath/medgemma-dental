{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    SiglipModel,\n",
        "    SiglipConfig,\n",
        "    PreTrainedModel,\n",
        "    AutoModel,\n",
        "    AutoProcessor,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "import os\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"justacoderwhocodes/dental_binary_treatment_classification\", split=\"train\")\n",
        "print(f\"Total dataset size: {len(dataset)}\")\n",
        "\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = split[\"train\"]\n",
        "val_ds = split[\"test\"]\n",
        "\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
        "\n",
        "label_map = {\"no_treatment\": 0, \"treatment\": 1}\n",
        "inv_label_map = {0: \"no_treatment\", 1: \"treatment\"}\n",
        "\n",
        "print(f\"Label distribution:\")\n",
        "print(f\"Train: {train_ds.features['label'].str2int(train_ds['label'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processor = AutoProcessor.from_pretrained(\"google/medsiglip-448\")\n",
        "medsiglip = SiglipModel.from_pretrained(\"google/medsiglip-448\", torch_dtype=torch.bfloat16)\n",
        "medsiglip.eval()\n",
        "\n",
        "for param in medsiglip.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(f\"MedSigLip parameters: {sum(p.numel() for p in medsiglip.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in medsiglip.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MedSigLipBinaryClassifierConfig(SiglipConfig):\n",
        "    model_type = \"medsiglip_binary_classifier\"\n",
        "    \n",
        "    def __init__(self, num_classes: int = 2, hidden_dim: int = 1152, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "class MedSigLipBinaryClassifier(PreTrainedModel):\n",
        "    config_class = MedSigLipBinaryClassifierConfig\n",
        "    base_model_prefix = \"vision_model\"\n",
        "    \n",
        "    def __init__(self, config: MedSigLipBinaryClassifierConfig):\n",
        "        super().__init__(config)\n",
        "        self.num_classes = config.num_classes\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        \n",
        "        self.vision_model = SiglipModel(config)\n",
        "        self.vision_model.eval()\n",
        "        \n",
        "        for param in self.vision_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.classifier = nn.Linear(config.hidden_dim, config.num_classes)\n",
        "        self.post_init()\n",
        "    \n",
        "    def forward(self, pixel_values: torch.Tensor, labels: Optional[torch.Tensor] = None):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.vision_model(pixel_values=pixel_values)\n",
        "            pooled = outputs.pooler_output\n",
        "        logits = self.classifier(pooled)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "        \n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "print(\"Custom HuggingFace-compatible model class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = MedSigLipBinaryClassifierConfig.from_pretrained(\"google/medsiglip-448\")\n",
        "model = MedSigLipBinaryClassifier(config)\n",
        "\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DentalClassDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset, processor, label_map):\n",
        "        self.dataset = hf_dataset\n",
        "        self.processor = processor\n",
        "        self.label_map = label_map\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        inputs = self.processor(images=item[\"image\"], return_tensors=\"pt\")\n",
        "        return {\n",
        "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
        "            \"labels\": self.label_map[item[\"label\"]]\n",
        "        }\n",
        "\n",
        "train_dataset = DentalClassDataset(train_ds, processor, label_map)\n",
        "val_dataset = DentalClassDataset(val_ds, processor, label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(model, dataset, inv_label_map, num_samples=4):\n",
        "    model.eval()\n",
        "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, sample_idx in enumerate(indices):\n",
        "        image = dataset.dataset[sample_idx][\"image\"]\n",
        "        inputs = processor(images=image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(pixel_values=inputs[\"pixel_values\"])\n",
        "            logits = outputs[\"logits\"].squeeze(0)\n",
        "            pred = torch.argmax(logits).item()\n",
        "        \n",
        "        actual = dataset[sample_idx][\"labels\"]\n",
        "        pred_label = inv_label_map[pred]\n",
        "        actual_label = inv_label_map[actual]\n",
        "        status = \"\\u2705\" if pred == actual else \"\\u274c\"\n",
        "        \n",
        "        axes[idx].imshow(image)\n",
        "        axes[idx].set_title(f\"Pred: {pred_label}\\nActual: {actual_label} {status}\")\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    model.train()\n",
        "\n",
        "print(\"Visualization function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VizCallback(TrainerCallback):\n",
        "    def __init__(self, model, dataset, inv_label_map, every_n_steps=200):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.inv_label_map = inv_label_map\n",
        "        self.every_n_steps = every_n_steps\n",
        "    \n",
        "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
        "        if state.global_step % self.every_n_steps == 0 and state.global_step > 0:\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Step {state.global_step} - Visualization\")\n",
        "            print(f\"{'='*50}\")\n",
        "            visualize_predictions(model, self.dataset, self.inv_label_map)\n",
        "    \n",
        "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Epoch {state.epoch} - Visualization\")\n",
        "        print(f\"{'='*50}\")\n",
        "        visualize_predictions(model, self.dataset, self.inv_label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./medsiglip-dental-classifier\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=50,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"justacoderwhocodes/medsiglip-dental-classifier\",\n",
        "    bf16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "print(\"Training config set up\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        VizCallback(model, val_dataset, inv_label_map, every_n_steps=200)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Trainer configured. Starting training...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model class code to enable AutoModel loading with trust_remote_code=True\n",
        "model_code = '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "from transformers import SiglipModel, SiglipConfig, PreTrainedModel\n",
        "\n",
        "class MedSigLipBinaryClassifierConfig(SiglipConfig):\n",
        "    model_type = \"medsiglip_binary_classifier\"\n",
        "    \n",
        "    def __init__(self, num_classes: int = 2, hidden_dim: int = 1152, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "class MedSigLipBinaryClassifier(PreTrainedModel):\n",
        "    config_class = MedSigLipBinaryClassifierConfig\n",
        "    base_model_prefix = \"vision_model\"\n",
        "    \n",
        "    def __init__(self, config: MedSigLipBinaryClassifierConfig):\n",
        "        super().__init__(config)\n",
        "        self.num_classes = config.num_classes\n",
        "        self.hidden_dim = config.hidden_dim\n",
        "        \n",
        "        self.vision_model = SiglipModel(config)\n",
        "        self.vision_model.eval()\n",
        "        \n",
        "        for param in self.vision_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.classifier = nn.Linear(config.hidden_dim, config.num_classes)\n",
        "        self.post_init()\n",
        "    \n",
        "    def forward(self, pixel_values: torch.Tensor, labels: Optional[torch.Tensor] = None):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.vision_model(pixel_values=pixel_values)\n",
        "            pooled = outputs.pooler_output\n",
        "        logits = self.classifier(pooled)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "        \n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "'''\n",
        "\n",
        "with open(\"modeling_medsiglip_classifier.py\", \"w\") as f:\n",
        "    f.write(model_code)\n",
        "\n",
        "trainer.push_to_hub()\n",
        "\n",
        "print(\"Model pushed to HuggingFace!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "loaded_model = AutoModel.from_pretrained(\n",
        "    \"justacoderwhocodes/medsiglip-dental-classifier\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "processor_loaded = AutoProcessor.from_pretrained(\"justacoderwhocodes/medsiglip-dental-classifier\")\n",
        "loaded_model.eval()\n",
        "\n",
        "print(\"Model loaded with AutoModel! Ready for inference.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_predictions(loaded_model, val_dataset, inv_label_map, num_samples=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference_example(image):\n",
        "    inputs = processor_loaded(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(loaded_model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = loaded_model(pixel_values=inputs[\"pixel_values\"])\n",
        "        logits = outputs[\"logits\"].squeeze(0)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        pred = torch.argmax(probs).item()\n",
        "        \n",
        "    label = inv_label_map[pred]\n",
        "    confidence = probs[pred].item()\n",
        "    \n",
        "    return label, confidence\n",
        "\n",
        "sample_image = val_ds[0][\"image\"]\n",
        "label, conf = inference_example(sample_image)\n",
        "print(f\"Predicted: {label} (confidence: {conf:.2%})\")\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(sample_image)\n",
        "plt.title(f\"Prediction: {label}\\nConfidence: {conf:.2%}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REPO_ID = \"justacoderwhocodes/medsiglip-dental-classifier\"\n",
        "print(\"\\nUsage for loading the model:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"from transformers import AutoModel, AutoProcessor\")\n",
        "print(f\"model = AutoModel.from_pretrained('{REPO_ID}', trust_remote_code=True)\")\n",
        "print(f\"processor = AutoProcessor.from_pretrained('{REPO_ID}')\")\n",
        "print(\"\\nTo run inference:\")\n",
        "print(\"inputs = processor(images=image, return_tensors='pt')\")\n",
        "print(\"outputs = model(pixel_values=inputs['pixel_values'])\")\n",
        "print(\"logits = outputs['logits']\")\n",
        "print(\"pred = torch.argmax(logits, dim=-1)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "nbformat_minor": 4,
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
